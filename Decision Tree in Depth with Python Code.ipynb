{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "Decision tree is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator in input variables.\n",
    "\n",
    "\n",
    "### Terminology\n",
    "\n",
    "* Root Node : It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "* Splitting :  It is a process of dividing a node into two or more sub-nodes.\n",
    "* Decision Node : When a sub-node splits into further sub-nodes\n",
    "* Leaf/ Terminal Node: Nodes do not split or the last node.\n",
    "* Pruning: When we remove sub-nodes of a decision node or can say opposite process of splitting.\n",
    "* Branch / Sub-Tree: A sub section of entire tree is called branch or sub-tree.\n",
    "* Parent and Child Node: A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node.\n",
    "    **Pruning is one of the technique used tackle overfitting.**\n",
    "![title](images/Capture10.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b10afa47-4287-499f-803f-a2b903c09c3a"
    }
   },
   "source": [
    "Using the decision algorithm, we start at the tree root and split the data on the feature that results in the largest information gain (IG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "931e584c-ab69-4f83-ae26-b0032860065c"
    }
   },
   "source": [
    "In an iterative process, we can then repeat this splitting procedure at each child node until the leaves are **pure**. This means that the samples at each node all belong to the **same class**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "78dcb4af-4780-42db-9b8a-08f63f09cdbb"
    }
   },
   "source": [
    "In practice, this can result in a very deep tree with many nodes, which can easily lead to **overfitting**. Thus, we typically want to **prune** the tree by setting a limit for the maximal depth of the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Decision Trees\n",
    "\n",
    "Types of decision tree is based on the type of target variable we have. It can be of two types:\n",
    "\n",
    "- Categorical Variable Decision Tree: Decision Tree which has categorical target variable then it called as categorical variable decision tree. Example:- In above scenario of student problem, where the target variable was “Student will play cricket or not” i.e. YES or NO.\n",
    "\n",
    "- Continuous Variable Decision Tree: Decision Tree has continuous target variable then it is called as Continuous Variable Decision Tree.\n",
    "\n",
    "Example:- Let’s say we have a problem to predict whether a customer will pay his renewal premium with an insurance company (yes/ no). Here we know that income of customer is a significant variable but insurance company does not have income details for all customers. Now, as we know this is an important variable, then we can build a decision tree to predict customer income based on occupation, product and various other variables. In this case, we are predicting values for continuous variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "\n",
    "* Easy to Understand: Decision tree output is very easy to understand even for people from non-analytical background. It does not require any statistical knowledge to read and interpret them. Its graphical representation is very intuitive and users can easily relate their hypothesis.\n",
    "* Useful in Data exploration: Decision tree is one of the fastest way to identify most significant variables and relation between two or more variables. With the help of decision trees, we can create new variables / features that has better power to predict target variable. You can refer article (Trick to enhance power of regression model) for one such trick.  It can also be used in data exploration stage. For example, we are working on a problem where we have information available in hundreds of variables, there decision tree will help to identify most significant variable.\n",
    "* Less data cleaning required: It requires less data cleaning compared to some other modeling techniques. It is not influenced by outliers and missing values to a fair degree.\n",
    "* Data type is not a constraint: It can handle both numerical and categorical variables.\n",
    "* Non Parametric Method: Decision tree is considered to be a non-parametric method. This means that decision trees have no assumptions about the space distribution and the classifier structure.\n",
    " \n",
    "### Disadvantages\n",
    "\n",
    "* Over fitting: Over fitting is one of the most practical difficulty for decision tree models. This problem gets solved by setting constraints on model parameters and pruning (discussed in detailed below).\n",
    "* Not fit for continuous variables: While working with continuous numerical variables, decision tree looses information when it categorizes variables in different categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Trees vs Classification Trees\n",
    "\n",
    "Both the trees work almost similar to each other, let’s look at the primary differences & similarity between classification and regression trees:\n",
    "\n",
    "* Regression trees are used when dependent variable is continuous. Classification trees are used when dependent variable is categorical.\n",
    "* In case of regression tree, the value obtained by terminal nodes in the training data is the mean response of observation falling in that region. Thus, if an unseen data observation falls in that region, we’ll make its prediction with mean value.\n",
    "* In case of classification tree, the value (class) obtained by terminal node in the training data is the mode of observations falling in that region. Thus, if an unseen data observation falls in that region, we’ll make its prediction with mode value.\n",
    "* Both the trees divide the predictor space (independent variables) into distinct and non-overlapping regions. For the sake of simplicity, you can think of these regions as high dimensional boxes or boxes.\n",
    "* Both the trees follow a top-down greedy approach known as recursive binary splitting. We call it as ‘top-down’ because it begins from the top of tree when all the observations are available in a single region and successively splits the predictor space into two new branches down the tree. It is known as ‘greedy’ because, the algorithm cares (looks for best variable available) about only the current split, and not about future splits which will lead to a better tree.\n",
    "* This splitting process is continued until a user defined stopping criteria is reached. For example: we can tell the the algorithm to stop once the number of observations per node becomes less than 50.\n",
    "* In both the cases, the splitting process results in fully grown trees until the stopping criteria is reached. But, the fully grown tree is likely to overfit data, leading to poor accuracy on unseen data. This bring ‘pruning’. Pruning is one of the technique used tackle overfitting. We’ll learn more about it in following section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does a tree decide where to split?\n",
    "\n",
    "* The decision of making strategic splits heavily affects a tree’s accuracy. The decision criteria is different for classification and regression trees.\n",
    "\n",
    "* Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.\n",
    "\n",
    "* The algorithm selection is also based on type of target variables. Let’s look at the four most commonly used algorithms in decision tree:\n",
    "\n",
    " \n",
    "\n",
    "#### Gini Index\n",
    "\n",
    "Gini index says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure.\n",
    "\n",
    "* It works with categorical target variable “Success” or “Failure”.\n",
    "* It performs only Binary splits\n",
    "* Higher the value of Gini higher the homogeneity.\n",
    "* CART (Classification and Regression Tree) uses Gini method to create binary splits.\n",
    "\n",
    "###### Steps to Calculate Gini for a split\n",
    "\n",
    "* Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure (p^2+q^2).\n",
    "* Calculate Gini for split using weighted Gini score of each node of that split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Gain:\n",
    "\n",
    " Information theory is a measure to define this degree of disorganization in a system known as Entropy. If the sample is completely homogeneous, then the entropy is zero and if the sample is an equally divided (50% – 50%), it has entropy of one.\n",
    " \n",
    " Entropy can be calculated using formula:\n",
    " \n",
    " Entropy = -plog2p -qlog2q\n",
    "\n",
    "Here p and q is probability of success and failure respectively in that node. Entropy is also used with categorical target variable. It chooses the split which has lowest entropy compared to parent node and other splits. The lesser the entropy, the better it is.\n",
    "\n",
    "Steps to calculate entropy for a split:\n",
    "\n",
    "* Calculate entropy of parent node\n",
    "* Calculate entropy of each individual node of split and calculate weighted average of all sub-nodes available in split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a3f6b6a9-0710-4509-bf04-ffd87d2d9457"
    }
   },
   "source": [
    "### Maximizing information gain\n",
    "\n",
    "In order to split the nodes for most informative features, we need to define an objective function that we want to optimize. Here,our objective function is to maximize the information gain at each split, which we define as follows:\n",
    "\n",
    "![title](images/Capture.PNG)\n",
    "\n",
    "**f** - is the feature to perform the split\n",
    "\n",
    "**$D_p$** - dataset of the parent\n",
    "\n",
    "**$D_j$** - dataset of the child of node *j*\n",
    "\n",
    "**I** - Impurity Measure\n",
    "\n",
    "**$N_p$** - total number of samples at the parent node\n",
    "\n",
    "**$N_j$** - number of samples in *j*th child node\n",
    "\n",
    "So, Information gain is simply the difference between the impurity of the parent node ***I($D_p$)***  and the sum of the child node impurities.\n",
    "\n",
    "**lower the impurity** of the child nodes, the larger the **information gain**.\n",
    "\n",
    "for simplicity and to reduce the combinatorial search space, most libraries (including scikit-learn) implement binary decision trees. This means that each parent node is split into two child nodes, D$_l$$_e$$_f$$_t$ and D$_r$$_i$$_g$$_h$$_t$ :\n",
    "![title](images/Capture1.PNG)\n",
    "\n",
    "There are three **impurity measures** or splitting criteria that are commonly used in binary decision trees are: \n",
    "\n",
    "1. **Entropy  ($I_H$)**\n",
    "\n",
    "2. **Gini index  ($I_G$)**\n",
    "\n",
    "3. **Classification error  ($I_E$)**.\n",
    "\n",
    "Let's start with the definition of **Entropy** for all non-empty classes\n",
    "![title](images/Capture6.PNG)\n",
    "A graph of entropy is given in fig.\n",
    "\n",
    "Suppose that we have +ve and -ve examples of some features. If all the examples are +ve, than we don't get any extra information from knowing the value of feature since, whatever the value of feature, the example will be positive.Thus the entropy of that feature is 0. However , if feature separates the example into 50% positive and 50% negative, then the amount of entropy is at a maximum.(and that feature is very important to us)\n",
    "\n",
    "![title](images/Capture2.PNG)\n",
    "\n",
    "Here, ***p(i/t)*** is the proportion of the samples that belongs to class c for a particular node t. The entropy is therefore 0 if all samples at a node belong to the same class, and the entropy is maximal if we have a uniform class distribution.\n",
    "\n",
    "Therefore, we can say that the entropy criterion attempts to maximize the mutual information in the tree.\n",
    "\n",
    "Intuitively, the **Gini index** can be understood as a criterion to minimize the probability of misclassification:\n",
    "![title](images/Capture3.PNG)\n",
    "\n",
    "However, in practice both the Gini index and entropy typically yield very similar results and it is often not worth spending much time on evaluating trees using different impurity criteria rather than experimenting with different pruning cut-offs.\n",
    "\n",
    "Another impurity measure is the **Classification error**:\n",
    "![title](images/Capture4.PNG)\n",
    "\n",
    "This is a useful criterion for pruning but not recommended for growing a decision tree, since it is less sensitive to changes in the class probabilities of the nodes.\n",
    "\n",
    "Ex: Lets take a example ![title](images/Capture5.PNG)\n",
    "\n",
    "We start with a dataset $D_p$ at the parent node $D_p$ that consists of 40 samples from class 1 and 40 samples from class 2 that we split into two datasets D$_l$$_e$$_f$$_t$ and D$_r$$_i$$_g$$_h$$_t$,respectively. The information gain using the **classification error** as a splitting criterion would be the same **(IG$_E$ = 0.25)** in both scenario A and B:\n",
    "\n",
    "I$_E$(D$_P$) = 1 - 0.5 = 0.5\n",
    "\n",
    "A : I$_E$(D$_l$$_e$$_f$$_t$) = 1 - 3/4 = 0.25\n",
    "\n",
    "A : I$_E$(D$_r$$_i$$_g$$_h$$_t$) = 1 - 3/4 = 0.25\n",
    "\n",
    "A ; $IG_E$ = 0.5 - (4/8)*0.25 - (4/8)*0.25 = 0.25\n",
    "\n",
    "**For B**\n",
    "\n",
    "B : I$_E$(D$_l$$_e$$_f$$_t$) = 1 - 4/6 = 1/3\n",
    "\n",
    "B : I$_E$(D$_r$$_i$$_g$$_h$$_t$) = 1 - 1 = 0\n",
    "\n",
    "B ; $IG_E$ = 0.5 - (6/8)*(1/3) - 0 = 0.25\n",
    "\n",
    "However, the **Gini index** would favor the split in scenario B(IG$_G$ = 0.16) over scenario, A(IG$_G$ = 0.125) which is indeed more pure:\n",
    "\n",
    "I$_G$(D$_P$) = 1 - (0.5$^2$ + 0.5$^2$)  = 0.5\n",
    "\n",
    "A : I$_G$(D$_l$$_e$$_f$$_t$) = 1 - ((3/4)$^2$ + (1/4)$^2$) = 0.375\n",
    "\n",
    "A : I$_G$(D$_r$$_i$$_g$$_h$$_t$) = 1 - ((1/4)$^2$ + (3/4)$^2$) = 0.375\n",
    "\n",
    "A ; $IG_G$ = 0.5 - (4/8)(0.375) - (4/8)(0.375) = 0.125\n",
    "\n",
    "**For B**\n",
    "\n",
    "B : I$_G$(D$_l$$_e$$_f$$_t$) = 1 - ((2/6)$^2$ + (4/6)$^2$) = 0.4\n",
    "\n",
    "B : I$_G$(D$_r$$_i$$_g$$_h$$_t$) = 1 - (1$^2$ + 0$^2$) = 0\n",
    "\n",
    "B ; $IG_G$ = 0.5 - (6/8)*(0.4) - 0 = 0.16\n",
    "\n",
    "Similarly, the **Entropy criterion** would favor scenario B(IG$_H$ = 0.19) over\n",
    "scenario A(IG$_H$ = 0.31):\n",
    "\n",
    "I$_H$(D$_P$) = - (0.5 log$_2$(0.5) + 0.5 log$_2$(0.5))  = 1\n",
    "\n",
    "A : I$_H$(D$_l$$_e$$_f$$_t$) = -((3/4) log$_2$(3/4) + (1/4) log$_2$(1/4)) = 0.81\n",
    "\n",
    "A : I$_H$(D$_r$$_i$$_g$$_h$$_t$) = -((1/4) log$_2$(1/4) + (3/4) log$_2$(3/4)) = 0.81\n",
    "\n",
    "A ; $IG_H$ = 1 - (4/8)(0.81) - (4/8)(0.81) = 0.19\n",
    "\n",
    "**For B**\n",
    "\n",
    "B : I$_H$(D$_l$$_e$$_f$$_t$) = -((2/6) log$_2$(2/6) + (4/6) log$_2$(4/6)) = 0.92\n",
    "\n",
    "B : I$_H$(D$_r$$_i$$_g$$_h$$_t$) = 0\n",
    "\n",
    "B ; $IG_H$ = 1 - (6/8)*(0.92) - 0 = 0.31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-Square\n",
    "\n",
    "It is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node. We measure it by sum of squares of standardized differences between observed and expected frequencies of target variable.\n",
    "\n",
    "* It works with categorical target variable “Success” or “Failure”.\n",
    "* It can perform two or more splits.\n",
    "* Higher the value of Chi-Square higher the statistical significance of differences between sub-node and Parent node.\n",
    "* Chi-Square of each node is calculated using formula,\n",
    "* Chi-square = ((Actual – Expected)^2 / Expected)^1/2\n",
    "* It generates tree called CHAID (Chi-square Automatic Interaction Detector)\n",
    "\n",
    "##### Steps to Calculate Chi-square for a split:\n",
    "\n",
    "* Calculate Chi-square for individual node by calculating the deviation for Success and Failure both\n",
    "* Calculated Chi-square of Split using Sum of all Chi-square of success and Failure of each node of the split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6aa80a14-dad5-49fd-9113-fad81191ce87"
    }
   },
   "source": [
    "## Dealing with Continuous Variables\n",
    "\n",
    "One thing that we have not yet discussed is how to deal with continuous variables, we have only considered those with discrete sets of feature values. The simplest solution is to diseretise the continuous variable. However, it is also possible to leave it continuous and modify the algorithm. For a continuous variable there is not just one place to split it: the variable can be broken between any pair of datapoints, as shown in fig.\n",
    "\n",
    "![title](images/Capture7.PNG)\n",
    "\n",
    "The trees that these algorithms make are all univariate trees, because they pick one feature (dimension) at a time and split according to that one. There are also algorithms that make multivariate trees by picking combinations of features. Fig. shows the idea. Given a dataset that contains three classes, the algorithm picks a feature and value for that feature to split the remaining data into two. \n",
    "\n",
    "![title](images/Capture8.PNG)\n",
    "\n",
    "\n",
    "As the outputs are continuous, so a regression model is appropriate. So, we will choose sum-of-squares-error.\n",
    "\n",
    "We can then pick the feature that has the split point that provides the best sum-of-squares error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the key parameters of tree modeling and how can we avoid over-fitting in decision trees?\n",
    "\n",
    "Overfitting is one of the key challenges faced while modeling decision trees. If there is no limit set of a decision tree, it will give you 100% accuracy on training set because in the worse case it will end up making 1 leaf for each observation. Thus, preventing overfitting is pivotal while modeling a decision tree and it can be done in 2 ways:\n",
    "\n",
    "Setting constraints on tree size\n",
    "> Tree pruning\n",
    "> Lets discuss both of these briefly.\n",
    "\n",
    "##### Setting Constraints on Tree Size\n",
    "\n",
    "This can be done by using various parameters which are used to define a tree. First, lets look at the general structure of a decision tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints on Tree Size\n",
    "\n",
    "![title](images/Capture11.PNG)\n",
    "\n",
    "* Minimum samples for a node split\n",
    "* Minimum samples for a terminal node (leaf)\n",
    "* Maximum depth of tree (vertical depth)\n",
    "* Maximum number of terminal nodes\n",
    "* Maximum features to consider for split\n",
    "\n",
    "\n",
    "The parameters used for defining a tree are further explained below. The parameters described below are irrespective of tool. It is important to understand the role of parameters used in tree modeling. These parameters are available in R & Python.\n",
    "\n",
    "* Minimum samples for a node split\n",
    "> Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.\n",
    "> Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "> Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "* Minimum samples for a terminal node (leaf)\n",
    "> Defines the minimum samples (or observations) required in a terminal node or leaf.\n",
    "> Used to control over-fitting similar to min_samples_split.\n",
    "> Generally lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in majority will be very small.\n",
    "* Maximum depth of tree (vertical depth)\n",
    "> The maximum depth of a tree.\n",
    "> Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "> Should be tuned using CV.\n",
    "* Maximum number of terminal nodes\n",
    "> The maximum number of terminal nodes or leaves in a tree.\n",
    "> Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "* Maximum features to consider for split\n",
    "> The number of features to consider while searching for a best split. These will be randomly selected.\n",
    "> As a thumb-rule, square root of the total number of features works great but we should check upto 30-40% of the total number of features.\n",
    "> Higher values can lead to over-fitting but depends on case to case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Pruning\n",
    "\n",
    "As discussed earlier, the technique of setting constraint is a greedy-approach. In other words, it will check for the best split instantaneously and move forward until one of the specified stopping condition is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are tree based models better than linear models?\n",
    "\n",
    "* “If I can use logistic regression for classification problems and linear regression for regression problems, why is there a need to use trees”? Many of us have this question. And, this is a valid one too.\n",
    "\n",
    "Actually, you can use any algorithm. It is dependent on the type of problem you are solving. Let’s look at some key factors which will help you to decide which algorithm to use:\n",
    "\n",
    "* If the relationship between dependent & independent variable is well approximated by a linear model, linear regression will outperform tree based model.\n",
    "* If there is a high non-linearity & complex relationship between dependent & independent variables, a tree model will outperform a classical regression method.\n",
    "* If you need to build a model which is easy to explain to people, a decision tree model will always do better than a linear model. Decision tree models are even simpler to interpret than linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Decision Tree in Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "\n",
    "model = tree.DecisionTreeClassifier(criterion='gini') \n",
    "\n",
    "#for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  \n",
    "#model = tree.DecisionTreeRegressor() for regression\n",
    "#Train the model using the training sets and check score\n",
    "\n",
    "model.fit(X, y)\n",
    "model.score(X, y)\n",
    "\n",
    "#Predict Output\n",
    "\n",
    "predicted= model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice feature in scikit-learn is that it allows us to export the decision tree as a .dot file after training, which we can visualize using the GraphViz program. This program is freely available at http://www.graphviz.org and supported by Linux, Windows, and Mac OS X.\n",
    "\n",
    "First, we create the .dot file via scikit-learn using the export_graphviz function from the tree submodule, as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree,out_file='tree.dot',feature_names=['petal length', 'petal width'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have installed GraphViz on our computer, we can convert the tree.dot file into a PNG file by executing the following command from the command line in the location where we saved the tree.dot file:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dot -Tpng tree.dot -o tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it will look like something this:\n",
    "![title](images/Capture12.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d1678790-ba66-4008-86f1-a4f00a378858"
    }
   },
   "source": [
    "## Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are ensemble methods in tree based modeling ?\n",
    "\n",
    "The literary meaning of word ‘ensemble’ is group. Ensemble methods involve group of predictive models to achieve a better accuracy and model stability. Ensemble methods are known to impart supreme boost to tree based models.\n",
    "\n",
    "Like every other model, a tree based model also suffers from the plague of bias and variance. Bias means, ‘how much on an average are the predicted values different from the actual value.’ Variance means, ‘how different will the predictions of the model be at the same point if different samples are taken from the same population’.\n",
    "\n",
    "You build a small tree and you will get a model with low variance and high bias. How do you manage to balance the trade off between bias and variance ?\n",
    "\n",
    "Normally, as you increase the complexity of your model, you will see a reduction in prediction error due to lower bias in the model. As you continue to make your model more complex, you end up over-fitting your model and your model will start suffering from high variance.\n",
    "\n",
    "A champion model should maintain a balance between these two types of errors. This is known as the trade-off management of bias-variance errors. Ensemble learning is one way to execute this trade off analysis.\n",
    "\n",
    "Some of the commonly used ensemble methods include: Bagging, Boosting and Stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "97f707b7-33b1-432b-a5f4-2690e4dd199c"
    }
   },
   "source": [
    "### Bagging \n",
    "\n",
    "Bagging is bootstrap aggregating. Fit classification or regression models to bootstrap samples from the data and combine by voting (classification) or averaging (regression). \n",
    "\n",
    "The simplest method of combining classifiers is known as bagging, which stands for *bootstrap aggregating*, the statistical description of the method. \n",
    "\n",
    "A bootstrap sample is a sample taken from the original dataset with replace-ment, so that we may get some data several times and others not at all.\n",
    "\n",
    "The bootstrap sample is the same size as the original, and lots and lots of these samples are taken: B of them, where B is at least 50, and could even be in the thousands.\n",
    "\n",
    "![title](images/Capture13.PNG)\n",
    "\n",
    "\n",
    "* Bagging puts most of its effort into ensuring that different classifiers see different data( different samples of data) which is different than boosting where data stays the same but its importance changes with different classifiers.\n",
    "\n",
    "* Here the number of models built is not a hyper-parameters. Higher number of models are always better or may give similar performance than lower numbers. It can be theoretically shown that the variance of the combined predictions are reduced to 1/n (n: number of classifiers) of the original variance, under some assumptions.\n",
    "\n",
    "\n",
    "#### Bagging has the following characteristics:\n",
    "\n",
    "* A bootstrap sample is chosen at random with replacement from the data. Some observations end up in the bootstrap sample more than once, while others are not included (\"out of bag\").\n",
    "\n",
    "* Bagging reduces the variance of the base learner but has limited effect on the bias.\n",
    "\n",
    "* The same greedy algorithm is used to create each tree, meaning that it is likely that the same or very similar trees will be correlated. This makes the predictions similar.\n",
    "\n",
    "#### The steps followed in bagging are:\n",
    "\n",
    "* Create Multiple DataSets:\n",
    "Sampling is done with replacement on the original data and new datasets are formed.\n",
    "The new data sets can have a fraction of the columns as well as rows, which are generally hyper-parameters in a bagging model\n",
    "Taking row and column fractions less than 1 helps in making robust models, less prone to overfitting\n",
    "* Build Multiple Classifiers:\n",
    "Classifiers are built on each data set.\n",
    "Generally the same classifier is modeled on each data set and predictions are made.\n",
    "* Combine Classifiers:\n",
    "The predictions of all the classifiers are combined using a mean, median or mode value depending on the problem at hand.\n",
    "The combined values are generally more robust than a single model.\n",
    "\n",
    "\n",
    "\n",
    "* There are various implementations of bagging models. **Random forest** is one of them.\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "RF grows a forest of many trees. It grows each tree on an independent bootstrap sample from the training data. Different from bagging, it forces the decision trees to be different by limiting the features that the greedy algorithm can evaluate at each split point when creating the tree. For example, the number of attrabutes to be considered for the split is limited to be the square root of the number of input features. This will make trees to be more different from each other (uncorrelated), resulting predicitons that are more diverse and a combined prediction that often has better performance.\n",
    "\n",
    "##### The algorithm\n",
    "\n",
    "At each node:\n",
    "* Select m variables at random out of all M possible variables.\n",
    "* Find the best split on the selected m variables.\n",
    "* Grow the trees to maximum depth (classification).\n",
    "* Vote/average the trees to get predicitons for new data.\n",
    "\n",
    "\n",
    "\n",
    "* In Random Forest, we grow multiple trees as opposed to a single decision tree.\n",
    "* To classify a new object based on attributes, each tree gives a classification and we say the tree “votes” for that class. The forest chooses the classification having the most votes (over all the trees in the forest) and in case of regression, it takes the average of outputs by different trees.\n",
    "* Each tree is grown to the largest extent possible and  there is no pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Random Forest in Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier #use RandomForestRegressor for regression problem\n",
    "\n",
    "#X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create Random Forest object\n",
    "\n",
    "model= RandomForestClassifier(n_estimators=1000)\n",
    "\n",
    "# Train the model using the training sets and check score\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "#Predict Output\n",
    "\n",
    "predicted= model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Advantages of Random Forest\n",
    "\n",
    "* This algorithm can solve both type of problems i.e. classification and regression and does a decent estimation at both fronts.\n",
    "* One of benefits of Random forest which excites me most is, the power of handle large data set with higher dimensionality. It can handle thousands of input variables and identify most significant variables so it is considered as one of the dimensionality reduction methods. Further, the model outputs Importance of variable, which can be a very handy feature (on some random data set).\n",
    "* It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.\n",
    "* It has methods for balancing errors in data sets where classes are imbalanced.\n",
    "* The capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection.\n",
    "* Random Forest involves sampling of the input data with replacement called as bootstrap sampling. Here one third of the data is not used for training and can be used to testing. These are called the out of bag samples. Error estimated on these out of bag samples is known as out of bag error. Study of error estimates by Out of bag, gives evidence to show that the out-of-bag estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set.\n",
    " \n",
    "\n",
    "##### Disadvantages of Random Forest\n",
    "\n",
    "* It surely does a good job at classification but not as good as for regression problem as it does not give precise continuous nature predictions. In case of regression, it doesn’t predict beyond the range in the training data, and that they may over-fit data sets that are particularly noisy.\n",
    "* Random Forest can feel like a black box approach for statistical modelers – you have very little control on what the model does. You can at best – try different parameters and random seeds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Boosting ? How does it work?\n",
    "\n",
    "Definition: The term ‘Boosting’ refers to a family of algorithms which converts weak learner to strong learners.\n",
    "\n",
    "Let’s understand this definition in detail by solving a problem of spam email identification:\n",
    "\n",
    "How would you classify an email as SPAM or not? Like everyone else, our initial approach would be to identify ‘spam’ and ‘not spam’ emails using following criteria. If:\n",
    "\n",
    "* Email has only one image file (promotional image), It’s a SPAM\n",
    "* Email has only link(s), It’s a SPAM\n",
    "* Email body consist of sentence like “You won a prize money of $ xxxxxx”, It’s a SPAM\n",
    "* Email from our official domain “AnalytxLabs.co.in” , Not a SPAM\n",
    "* Email from known source, Not a SPAM\n",
    "\n",
    "Above, we’ve defined multiple rules to classify an email into ‘spam’ or ‘not spam’. But, do you think these rules individually are strong enough to successfully classify an email? No.\n",
    "\n",
    "Individually, these rules are not powerful enough to classify an email into ‘spam’ or ‘not spam’. Therefore, these rules are called as weak learner.\n",
    "\n",
    "To convert weak learner to strong learner, we’ll combine the prediction of each weak learner using methods like:\n",
    "\n",
    "* Using average/ weighted average\n",
    "* Considering prediction has higher vote\n",
    "\n",
    "For example:  Above, we have defined 5 weak learners. Out of these 5, 3 are voted as ‘SPAM’ and 2 are voted as ‘Not a SPAM’. In this case, by default, we’ll consider an email as SPAM because we have higher(3) vote for ‘SPAM’.\n",
    "\n",
    " \n",
    "\n",
    "How does it work?\n",
    "\n",
    "Now we know that, boosting combines weak learner a.k.a. base learner to form a strong rule. An immediate question which should pop in your mind is, ‘How boosting identify weak rules?‘\n",
    "\n",
    "To find weak rule, we apply base learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.\n",
    "\n",
    "Here’s another question which might haunt you, ‘How do we choose different distribution for each round?’\n",
    "\n",
    "For choosing the right distribution, here are the following steps:\n",
    "\n",
    "Step 1:  The base learner takes all the distributions and assign equal weight or attention to each observation.\n",
    "\n",
    "Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.\n",
    "\n",
    "Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\n",
    "\n",
    "Finally, it combines the outputs from weak learner and creates  a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules.\n",
    "There are many boosting algorithms which impart additional boost to model’s accuracy. In this tutorial, we’ll learn about the two most commonly used algorithms i.e. Gradient Boosting (GBM) and XGboost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "If we take a collection of very poor learners, each performing only just better than chance, then by putting them together it is possible to make an ensemble learner that can perform arbritrarily well. So we just need lots of low quality learners, and a way to put them together usefully, and we can make a learner that will do very well. The principal algorithm of boosting is named *AdaBoost*.\n",
    "\n",
    "The algorithm was proposed as an improvement on the original 1990 boosting algorithm. In that algorithm, the training set was split into three. A classifier was trained on the first third, and then tested on the sec-ond third. All of the data that was misclassified during that testing was used to form a new dataset, along with an equally sized random selection of the data that was correctly classified. A second classifier was trained on this new dataset, and then both of the classifiers were tested on the final third of the dataset. If they both produced the same output, then that datapoint was ig-nored, otherwise the datapoint was added to yet another new dataset, which formed the training set for a third classifier\n",
    "\n",
    "### Adaboost\n",
    "\n",
    "The innovation that AdaBoost (which stands for adaptive boosting) uses is to give weights to each datapoint according to how difficult previous classifiers have found to get it correct. These weights are given to the classifier as part of the input when it is trained. \n",
    "\n",
    "At each iteration a new classifier is trained on the training set, with the weights that are applied to the training set for each datapoint being modified at each iteration accord-ing to how successfully that datapoint has been classified in the past. The weights are initially all set to the same value, 1/N, whore N is the number of datapoints in the training set.\n",
    "\n",
    "![title](images/Capture9.PNG)\n",
    "\n",
    "* As points are misclassified, so their weights increased in boosting(datapoint getting larger), which increase the *importance* of that data point, make the classifier to pay more attention to them.\n",
    "\n",
    "* Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules.\n",
    "\n",
    "* Two most commonly used algorithms i.e. **Gradient Boosting (GBM)** and **XGboost**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "1. Initialize the outcome\n",
    "\n",
    "2. Iterate from 1 to total number of trees\n",
    "\n",
    "  2.1 Update the weights for targets based on previous run (higher for the ones mis-classified)\n",
    "  \n",
    "  2.2 Fit the model on selected subsample of data\n",
    "  \n",
    "  2.3 Make predictions on the full set of observations \n",
    "  \n",
    "  2.4 Update the output with current results taking into account the learning rate\n",
    "  \n",
    "3. Return the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM Parameters\n",
    "\n",
    "* **Tree-Specific Parameters** : These affect each individual tree in the model.\n",
    "* **Boosting Parameters**: These affect the boosting operation in the model.\n",
    "* **Miscellaneous Parameters**: Other parameters for overall functioning.\n",
    "\n",
    "#### Tree-Specific Parameters\n",
    "1. **min_samples_split** : Defines the minimum number of samples which are required in a node to be considered for splitting.\n",
    "2. **min_samples_leaf** : Defines the minimum samples should required in a terminal node or leaf.\n",
    "3. **max_depth** : Defines the maximum depth of a tree.\n",
    "4. **max_leaf_nodes** : Defines the maximum number of terminal nodes or leaves in a tree.\n",
    "5. **max_features** : \n",
    "    * The number of features to consider while searching for a best split. These will be randomly selected.\n",
    "    * As a thumb-rule, square root of the total number of features works great but we should check upto 30-40% of the total number of features.\n",
    "    \n",
    "The parameters which we have considered so far will affect step 2.2, i.e. **model building**.\n",
    "\n",
    "#### Boosting Parameters\n",
    "1. **learning_rate** : This determines the impact of each tree on the final outcome (step 2.4). The learning parameter controls the magnitude of this change in the estimates. Lower values are generally preferred as they make the model robust.\n",
    "2. **n_estimators** : The number of sequential trees to be modeled (step 2).\n",
    "3. **subsample** : \n",
    "    * The fraction of observations to be selected for each tree. Selection is done by random sampling.\n",
    "    * Values slightly less than 1 make the model robust by reducing the variance.\n",
    "    * Typical values ~0.8 generally work fine but can be fine-tuned further.\n",
    "\n",
    "#### Miscellaneous Parameters\n",
    "1. **loss** : It refers to the loss function to be minimized in each split.\n",
    "2. **init** : This affects initialization of the output.\n",
    "3. **random_state** : The random number seed so that same random numbers are generated every time.\n",
    "4. **warm_start** : This parameter has an interesting application and can help a lot if used judicially.Using this, we can fit additional trees on previous fits of a model. It can save a lot of time and you should explore this option for advanced applications.\n",
    "5. **verbose** : The type of output to be printed when the model fits. The different values can be:\n",
    "        0: no output generated (default)\n",
    "        1: output generated for trees in certain intervals\n",
    "       >1: output generated for all trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for GBM in Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier #For Classification\n",
    "from sklearn.ensemble import GradientBoostingRegressor #For Regression\n",
    "#use GBM function\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "clf.fit(X_train, y_train)\n",
    "#Predict Output\n",
    "predicted= model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "We always need to find a way to find the best parameters given the training data. In order to do so, we need to define a so-called **objective function**,to measure the performance of the model given a certain set of parameters.\n",
    "\n",
    "        objective function  ~  Training Loss + Regularization\n",
    "\n",
    "A very important fact about objective functions is they must always contain two parts: **training loss** and **regularization**.\n",
    "![title](images/Capture14.PNG)\n",
    "\n",
    "where L is the training loss function, and Ω is the regularization term. The training loss measures how predictive our model is on training data. For example, a commonly used training loss is mean squared error.\n",
    "\n",
    "The **regularization term** controls the complexity of the model, which helps us to avoid overfitting.Lets see the fig.\n",
    "\n",
    "![title](images/Capture15.PNG)\n",
    "\n",
    "The general principle is we want both a simple and predictive model. The tradeoff between the two is also referred as bias-variance tradeoff in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Parameters\n",
    "\n",
    "* **General Parameters** : Guide the overall functioning\n",
    "* **Booster Parameters** : Guide the individual booster (tree/regression) at each step\n",
    "* **Learning Task Parameters** : Guide the optimization performed\n",
    "\n",
    "#### General Parameters\n",
    "\n",
    "1. **booster [default=gbtree]** : Select the type of model to run at each iteration. It has 2 options:\n",
    "                                    gbtree: tree-based models\n",
    "                                    gblinear: linear models\n",
    "2. **silent [default=0]** : When Silent mode is set to 1, i.e. no running messages will be printed.(0 recommended)\n",
    "3. **nthread [default to maximum number of threads available if not set]** : This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically.\n",
    "\n",
    "#### Booster Parameters\n",
    "\n",
    "1. **eta [default=0.3]** : Analogous to learning rate in GBM. Typical final values to be used: 0.01-0.2\n",
    "2. **min_child_weight [default=1]** : Defines the minimum sum of weights of all observations required in a child.\n",
    "3. **max_depth [default=6]** : same as GBM. Typical values: 3-10\n",
    "4. **max_leaf_nodes** : The maximum number of terminal nodes or leaves in a tree.\n",
    "5. **gamma [default=0]** : A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "6. **subsample [default=1]** : Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "7. **lambda [default=1]** : L2 regularization term on weights (analogous to Ridge regression). This used to handle the regularization part of XGBoost. It should be explored to reduce overfitting.\n",
    "8. **alpha [default=0]** : L1 regularization term on weight (analogous to Lasso regression). Can be used in case of very high dimensionality so that the algorithm runs faster when implemented.\n",
    "9. **scale_pos_weight [default=1]** : A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence.\n",
    "\n",
    "#### Learning Task Parameters\n",
    "\n",
    "1. **objective [default=reg:linear]** : This defines the loss function to be minimized. Mostly used values are:\n",
    "\n",
    "    * **binary:logistic** –logistic regression for binary classification, returns predicted probability (not class)\n",
    "    * **multi:softmax** –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "    * **multi:softprob** –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "\n",
    "2. **eval_metric [ default according to objective ]** : The metric to be used for validation data. The default values are rmse for regression and error for classification.Typical values are :\n",
    "\n",
    "    * **rmse** – root mean square error\n",
    "    * **mae** – mean absolute error\n",
    "    * **logloss** – negative log-likelihood\n",
    "    * **error** – Binary classification error rate (0.5 threshold)\n",
    "    * **merror** – Multiclass classification error rate\n",
    "    * **mlogloss** – Multiclass logloss\n",
    "    * **auc**: Area under the curve\n",
    "\n",
    "\n",
    "In Scikit-Learn the parameters names which will change are:\n",
    "\n",
    "                    eta –> learning_rate\n",
    "                    lambda –> reg_lambda\n",
    "                    alpha –> reg_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which is more powerful: GBM or Xgboost?\n",
    "\n",
    "I’ve always admired the boosting capabilities that xgboost algorithm. At times, I’ve found that it provides better result compared to GBM implementation, but at times you might find that the gains are just marginal. When I explored more about its performance and science behind its high accuracy, I discovered many advantages of Xgboost over GBM:\n",
    "\n",
    "* Regularization:\n",
    "Standard GBM implementation has no regularization like XGBoost, therefore it also helps to reduce overfitting.\n",
    "In fact, XGBoost is also known as ‘regularized boosting‘ technique.\n",
    "* Parallel Processing:\n",
    "XGBoost implements parallel processing and is blazingly faster as compared to GBM.\n",
    "But hang on, we know that boosting is sequential process so how can it be parallelized? We know that each tree can be built only after the previous one, so what stops us from making a tree using all cores? I hope you get where I’m coming from. Check this link out to explore further.\n",
    "XGBoost also supports implementation on Hadoop.\n",
    "* High Flexibility: \n",
    "XGBoost allow users to define custom optimization objectives and evaluation criteria.\n",
    "This adds a whole new dimension to the model and there is no limit to what we can do.\n",
    "* Handling Missing Values: \n",
    "XGBoost has an in-built routine to handle missing values.\n",
    "User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future.\n",
    "* Tree Pruning:\n",
    "A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm.\n",
    "XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.\n",
    "Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.\n",
    "* Built-in Cross-Validation :\n",
    "XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.\n",
    "This is unlike GBM where we have to run a grid-search and only a limited values can be tested.\n",
    "* Continue on Existing Model:\n",
    "User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications.\n",
    "GBM implementation of sklearn also has this feature so they are even on this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
